---
---
@inproceedings{10.1145/1869790.1869884,
author = {Lassoued, Yassine and Bouadjenek, Mohamed Reda and Boucelma, Omar and Lemos, Fernando and Bouzeghoub, Mokrane},
title = {GQBox: geospatial data quality assessment},
year = {2010},
isbn = {9781450304283},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1869790.1869884},
doi = {10.1145/1869790.1869884},
abstract = {In order to measure and assess the quality of GIS, there exist a sparse offer of tools, providing specific functions with their own interest but are not sufficient to deal with broader user's requirements. Interoperability of these tools remains a technical challenge because of the heterogeneity of their models and access patterns. On the other side, quality analysts require more and more integration facilities that allow them to consolidate and aggregate multiple quality measures acquired from different observations or data sources, in using/combining seamlessly different quality tools. Clearly, there is a gap between users's requirements and the spatial data quality market. This demo paper will illustrate GQBox, a geographic quality (tool)box. GQBox supplies a standards-based generic meta model that supports the definition of quality goals and metrics, and it provides a service-based infrastructure that allows interoperability among several quality tools.},
booktitle = {Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {534–535},
numpages = {2},
keywords = {web services, geospatial data quality},
location = {San Jose, California},
series = {GIS '10},
pdf={p534-lassoued.pdf},
selected={false},
preview={acm.png},
}

@inproceedings{10.1145/2009916.2010075,
author = {Bouadjenek, Mohamed Reda and Hacid, Hakim and Bouzeghoub, Mokrane and Daigremont, Johann},
title = {Personalized social query expansion using social bookmarking systems},
year = {2011},
isbn = {9781450307574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2009916.2010075},
doi = {10.1145/2009916.2010075},
abstract = {We propose a new approach for social and personalized query expansion using social structures in the Web 2.0. While focusing on social tagging systems, the proposed approach considers (i) the semantic similarity between tags composing a query, (ii) a social proximity between the query and the user profile, and (iii) on the fly, a strategy for expanding user queries. The proposed approach has been evaluated using a large dataset crawled from del.icio.us.},
booktitle = {Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1113–1114},
numpages = {2},
keywords = {social networks, social information retrieval, personalization},
location = {Beijing, China},
series = {SIGIR '11},
pdf={p1113-bouadjenek.pdf},
selected={false},
preview={acm.png},
}

@InProceedings{10.1007/978-3-642-39200-9_24,
author={Bouadjenek, Mohamed Reda
and Bennamane, Amyn
and Hacid, Hakim
and Bouzeghoub, Mokrane},
editor={Daniel, Florian
and Dolog, Peter
and Li, Qing},
title={Evaluation of Personalized Social Ranking Functions of Information Retrieval},
booktitle={Web Engineering},
year={2013},
publisher={Springer Berlin Heidelberg},
address={Berlin, Heidelberg},
pages={283--290},
abstract={There is currently a number of interesting research works performed in the area of bridging the gap between Social Networks and Information Retrieval (IR). This is mainly done by enhancing the IR process with social information. Hence, many approaches have been proposed to improve the ranking process by personalizing it using social features. In this paper, we review some of these ranking functions.},
isbn={978-3-642-39200-9},
pdf={ICWE2013.pdf},
selected={false},
preview={springer.png},
}

@inproceedings{10.1145/2484028.2484131,
author = {Bouadjenek, Mohamed Reda and Hacid, Hakim and Bouzeghoub, Mokrane},
title = {Sopra: a new social personalized ranking function for improving web search},
year = {2013},
isbn = {9781450320344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484028.2484131},
doi = {10.1145/2484028.2484131},
abstract = {We present in this paper a contribution to IR modeling by proposing a new ranking function called SoPRa that considers the social dimension of the Web. This social dimension is any social information that surrounds documents along with the social context of users. Currently, our approach relies on folksonomies for extracting these social contexts, but it can be extended to use any social meta-data, e.g. comments, ratings, tweets, etc. The evaluation performed on our approach shows its benefits for personalized search.},
booktitle = {Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {861–864},
numpages = {4},
keywords = {information retrieval, social networks},
location = {Dublin, Ireland},
series = {SIGIR '13},
pdf={sp086-bouadjenek.pdf},
selected={true},
preview={acm.png},
}

@inproceedings{10.1145/2484028.2484130,
author = {BOUADJENEK, Mohamed Reda and Hacid, Hakim and Bouzeghoub, Mokrane and Vakali, Athena},
title = {Using social annotations to enhance document representation for personalized search},
year = {2013},
isbn = {9781450320344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484028.2484130},
doi = {10.1145/2484028.2484130},
abstract = {In this paper, we present a contribution to IR modeling. We propose an approach that computes on the fly, a Personalized Social Document Representation (PSDR) of each document per user based on his social activities. The PSDRs are used to rank documents with respect to a query. This approach has been intensively evaluated on a large public dataset, showing significant benefits for personalized search.},
booktitle = {Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1049–1052},
numpages = {4},
keywords = {information retrieval, social networks},
location = {Dublin, Ireland},
series = {SIGIR '13},
pdf={sp085-bouadjenek.pdf},
selected={false},
preview={acm.png},
}

@inproceedings{10.1145/2487575.2487705,
author = {Bouadjenek, Mohamed Reda and Hacid, Hakim and Bouzeghoub, Mokrane},
title = {LAICOS: an open source platform for personalized social web search},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2487705},
doi = {10.1145/2487575.2487705},
abstract = {In this paper, we introduce LAICOS, a social Web search engine as a contribution to the growing area of Social Information Retrieval (SIR). Social information and personalization are at the heart of LAICOS. On the one hand, the social context of documents is added as a layer to their textual content traditionally used for indexing to provide Personalized Social Document Representations. On the other hand, the social context of users is used for the query expansion process using the Personalized Social Query Expansion framework (PSQE) proposed in our earlier works. We describe the different components of the system while relying on social bookmarking systems as a source of social information for personalizing and enhancing the IR process. We show how the internal structure of indexes as well as the query expansion process operated using social information.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1446–1449},
numpages = {4},
keywords = {social networks, social information retrieval, personalization, information retrieval},
location = {Chicago, Illinois, USA},
series = {KDD '13},
pdf={demo16-bouadjenek.pdf},
selected={false},
preview={acm.png},
}

@inproceedings{10.1145/2746090.2746092,
author = {Bouadjenek, Mohamed Reda and Sanner, Scott and Ferraro, Gabriela},
title = {A study of query reformulation for patent prior art search with partial patent applications},
year = {2015},
isbn = {9781450335225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2746090.2746092},
doi = {10.1145/2746090.2746092},
abstract = {Patents are used by legal entities to legally protect their inventions and represent a multi-billion dollar industry of licensing and litigation. In 2014, 326,033 patent applications were approved in the US alone -- a number that has doubled in the past 15 years and which makes prior art search a daunting, but necessary task in the patent application process. In this work, we seek to investigate the efficacy of prior art search strategies from the perspective of the inventor who wishes to assess the patentability of their ideas prior to writing a full application. While much of the literature inspired by the evaluation framework of the CLEF-IP competition has aimed to assist patent examiners in assessing prior art for complete patent applications, less of this work has focused on patent search with queries representing partial applications. In the (partial) patent search setting, a query is often much longer than in other standard IR tasks, e.g., the description section may contain hundreds or even thousands of words. While the length of such queries may suggest query reduction strategies to remove irrelevant terms, intentional obfuscation and general language used in patents suggests that it may help to expand queries with additionally relevant terms. To assess the trade-offs among all of these pre-application prior art search strategies, we comparatively evaluate a variety of partial application search and query reformulation methods. Among numerous findings, querying with a full description, perhaps in conjunction with generic (non-patent specific) query reduction methods, is recommended for best performance. However, we also find that querying with an abstract represents the best trade-off in terms of writing effort vs. retrieval efficacy (i.e., querying with the description sections only lead to marginal improvements) and that for such relatively short queries, generic query expansion methods help.},
booktitle = {Proceedings of the 15th International Conference on Artificial Intelligence and Law},
pages = {23–32},
numpages = {10},
keywords = {patent search, query reformulation},
location = {San Diego, California},
series = {ICAIL '15},
pdf={ICAIL2015.pdf},
selected={false},
preview={acm.png},
}

@inproceedings{10.1145/2766462.2767801,
author = {Golestan Far, Mona and Sanner, Scott and Bouadjenek, Mohamed Reda and Ferraro, Gabriela and Hawking, David},
title = {On Term Selection Techniques for Patent Prior Art Search},
year = {2015},
isbn = {9781450336215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2766462.2767801},
doi = {10.1145/2766462.2767801},
abstract = {In this paper, we investigate the influence of term selection on retrieval performance on the CLEF-IP prior art test collection, using the Description section of the patent query with Language Model (LM) and BM25 scoring functions. We find that an oracular relevance feedback system that extracts terms from the judged relevant documents far outperforms the baseline and performs twice as well on MAP as the best competitor in CLEF-IP 2010. We find a very clear term selection value threshold for use when choosing terms. We also noticed that most of the useful feedback terms are actually present in the original query and hypothesized that the baseline system could be substantially improved by removing negative query terms. We tried four simple automated approaches to identify negative terms for query reduction but we were unable to notably improve on the baseline performance with any of them. However, we show that a simple, minimal interactive relevance feedback approach where terms are selected from only the first retrieved relevant document outperforms the best result from CLEF-IP 2010 suggesting the promise of interactive methods for term selection in patent prior art search.},
booktitle = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {803–806},
numpages = {4},
keywords = {patent search, query reformulation},
location = {Santiago, Chile},
series = {SIGIR '15},
pdf={p803-golestan-far.pdf},
selected={false},
preview={acm.png},
}

@article{BOUADJENEK20161,
title = {Social networks and information retrieval, how are they converging? A survey, a taxonomy and an analysis of social information retrieval approaches and platforms},
journal = {Information Systems},
volume = {56},
pages = {1-18},
year = {2016},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2015.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S030643791500160X},
author = {Mohamed Reda Bouadjenek and Hakim Hacid and Mokrane Bouzeghoub},
keywords = {Information Retrieval, Social networks, Social Information Retrieval, Social search, Social recommendation},
abstract = {There is currently a number of research work performed in the area of bridging the gap between Information Retrieval (IR) and Online Social Networks (OSN). This is mainly done by enhancing the IR process with information coming from social networks, a process called Social Information Retrieval (SIR). The main question one might ask is What would be the benefits of using social information (no matter whether it is content or structure) into the information retrieval process and how is this currently done? With the growing number of efforts towards the combination of IR and social networks, it is necessary to build a clearer picture of the domain and synthesize the efforts in a structured and meaningful way. This paper reviews different efforts in this domain. It intends to provide a clear understanding of the issues as well as a clear structure of the contributions. More precisely, we propose (i) to review some of the most important contributions in this domain to understand the principles of SIR, (ii) a taxonomy to categorize these contributions, and finally, (iii) an analysis of some of these contributions and tools with respect to several criteria, which we believe are crucial to design an effective SIR approach. This paper is expected to serve researchers and practitioners as a reference to help them structuring the domain, position themselves and, ultimately, help them to propose new contributions or improve existing ones.},
pdf={IS-D-14-351.pdf},
selected={true},
preview={Elsevier.svg.png},
}

@article{BOUADJENEK2016614,
title = {PerSaDoR: Personalized social document representation for improving web search},
journal = {Information Sciences},
volume = {369},
pages = {614-633},
year = {2016},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2016.07.046},
url = {https://www.sciencedirect.com/science/article/pii/S0020025516305278},
author = {Mohamed Reda Bouadjenek and Hakim Hacid and Mokrane Bouzeghoub and Athena Vakali},
keywords = {Information retrieval, Social networks, Social information retrieval, Social search, Social recommendation},
abstract = {In this paper, we discuss a contribution towards the integration of social information in the index structure of an IR system. Since each user has his/her own understanding and point of view of a given document, we propose an approach in which the index model provides a Personalized Social Document Representation (PerSaDoR) of each document per user based on his/her activities in a social tagging system. The proposed approach relies on matrix factorization to compute the PerSaDoR of documents that match a query, at query time. The complexity analysis shows that our approach scales linearly with the number of documents that match the query, and thus, it can scale to very large datasets. PerSaDoR has been also intensively evaluated by an offline study and by a user survey operated on a large public dataset from delicious showing significant benefits for personalized search compared to state of the art methods.},
pdf={INS-D-15-2480.pdf},
selected={false},
preview={Elsevier.svg.png},
}

@article{10.1093/database/bax062,
    author = {Bouadjenek, Mohamed Reda and Verspoor, Karin},
    title = {Multi-field query expansion is effective for biomedical dataset retrieval},
    journal = {Database},
    volume = {2017},
    pages = {bax062},
    year = {2017},
    month = {09},
    abstract = {In the context of the bioCADDIE challenge addressing information retrieval of biomedical datasets, we propose a method for retrieval of biomedical data sets with heterogenous schemas through query reformulation. In particular, the method proposed transforms the initial query into a multi-field query that is then enriched with terms that are likely to occur in the relevant datasets. We compare and evaluate two query expansion strategies, one based on the Rocchio method and another based on a biomedical lexicon. We then perform a comprehensive comparative evaluation of our method on the bioCADDIE dataset collection for biomedical retrieval. We demonstrate the effectiveness of our multi-field query method compared to two baselines, with MAP improved from 0.2171 and 0.2669 to 0.2996. We also show the benefits of query expansion, where the Rocchio expanstion method improves the MAP for our two baselines from 0.2171 and 0.2669 to 0.335. We show that the Rocchio query expansion method slightly outperforms the one based on the biomedical lexicon as a source of terms, with an improvement of roughly 3\% for MAP. However, the query expansion method based on the biomedical lexicon is much less resource intensive since it does not require computation of any relevance feedback set or any initial execution of the query. Hence, in term of trade-off between efficiency, execution time and retrieval accuracy, we argue that the query expansion method based on the biomedical lexicon offers the best performance for a prototype biomedical data search engine intended to be used at a large scale. In the official bioCADDIE challenge results, although our approach is ranked seventh in terms of the infNDCG evaluation metric, it ranks second in term of P@10 and NDCG. Hence, the method proposed here provides overall good retrieval performance in relation to the approaches of other competitors. Consequently, the observations made in this paper should benefit the development of a Data Discovery Index prototype or the improvement of the existing one.},
    issn = {1758-0463},
    doi = {10.1093/database/bax062},
    url = {https://doi.org/10.1093/database/bax062},
    eprint = {https://academic.oup.com/database/article-pdf/doi/10.1093/database/bax062/22892713/bax062.pdf},
	pdf={bax062.pdf},
	selected={false},
	preview={oxford_university_press.jpg},
}

@article{BOUADJENEK2017229,
title = {Automated detection of records in biological sequence databases that are inconsistent with the literature},
journal = {Journal of Biomedical Informatics},
volume = {71},
pages = {229-240},
year = {2017},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2017.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S1532046417301399},
author = {Mohamed Reda Bouadjenek and Karin Verspoor and Justin Zobel},
keywords = {Data analysis, Data quality, Bioinformatics databases, Anomaly detection},
abstract = {We investigate and analyse the data quality of nucleotide sequence databases with the objective of automatic detection of data anomalies and suspicious records. Specifically, we demonstrate that the published literature associated with each data record can be used to automatically evaluate its quality, by cross-checking the consistency of the key content of the database record with the referenced publications. Focusing on GenBank, we describe a set of quality indicators based on the relevance paradigm of information retrieval (IR). Then, we use these quality indicators to train an anomaly detection algorithm to classify records as “confident” or “suspicious”. Our experiments on the PubMed Central collection show assessing the coherence between the literature and database records, through our algorithms, is an effective mechanism for assisting curators to perform data cleansing. Although fewer than 0.25% of the records in our data set are known to be faulty, we would expect that there are many more in GenBank that have not yet been identified. By automated comparison with literature they can be identified with a precision of up to 10% and a recall of up to 30%, while strongly outperforming several baselines. While these results leave substantial room for improvement, they reflect both the very imbalanced nature of the data, and the limited explicitly labelled data that is available. Overall, the obtained results show promise for the development of a new kind of approach to detecting low-quality and suspicious sequence records based on literature analysis and consistency. From a practical point of view, this will greatly help curators in identifying inconsistent records in large-scale sequence databases by highlighting records that are likely to be inconsistent with the literature.},
pdf={JBI-17-134.pdf},
selected={false},
preview={Elsevier.svg.png},
}

@inproceedings{10.1145/3132847.3133051,
author = {Bouadjenek, Mohamed Reda and Verspoor, Karin and Zobel, Justin},
title = {Learning Biological Sequence Types Using the Literature},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133051},
doi = {10.1145/3132847.3133051},
abstract = {We explore in this paper automatic biological sequence type classification for records in biological sequence databases. The sequence type attribute provides important information about the nature of a sequence represented in a record, and is often used in search to filter out irrelevant sequences. However, the sequence type attribute is generally a non-mandatory free-text field, and thus it is subject to many errors including typos, mis-assignment, and non-assignment. In GenBank, this problem concerns roughly 18\% of records, an alarming number that should worry the biocuration community. To address this problem of automatic sequence type classification, we propose the use of literature associated to sequence records as an external source of knowledge that can be leveraged for the classification task. We define a set of literature-based features and train a machine learning algorithm to classify a record into one of six primary sequence types. The main intuition behind using the literature for this task is that sequences appear to be discussed differently in scientific articles, depending on their type. The experiments we have conducted on the PubMed Central collection show that the literature is indeed an effective way to address this problem of sequence type classification. Our classification method reached an accuracy of 92.7\%, and substantially outperformed two baseline approaches used for comparison.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1991–1994},
numpages = {4},
keywords = {data quality, data cleansing, data analysis, biological databases},
location = {Singapore, Singapore},
series = {CIKM '17},
pdf={sp0228-bouadjenek.pdf},
selected={false},
preview={acm.png},
}

@article{Iman_Sanner_Bouadjenek_Xie_2017, 
title={A Longitudinal Study of Topic Classification on Twitter}, 
volume={11}, 
url={https://ojs.aaai.org/index.php/ICWSM/article/view/14934}, 
DOI={10.1609/icwsm.v11i1.14934}, 
abstractNote={ &lt;p&gt; Twitter represents a massively distributed information source over a kaleidoscope of topics ranging from social and political events to entertainment and sports news. While recent work has suggested that variations on standard classifiers can be effectively trained as topical filters (Lin, Snow, and Morgan 2011; Yang et al. 2014; Magdy and Elsayed 2014), there remain many open questions about the efficacy of such classification-based filtering approaches. For example, over a year or more after training, how well do such classifiers generalize to future novel topical content, and are such results stable across a range of topics? Furthermore, what features and feature classes are most critical for long-term classifier performance? To answer these questions, we collected a corpus of over 800 million English Tweets via the Twitter streaming API during 2013 and 2014 and learned topic classifiers for 10 diverse themes ranging from social issues to celebrity deaths to the “Iran nuclear deal”. The results of this long-term study of topic classifier performance provide a number of important insights, among them that (1) such classifiers can indeed generalize to novel topical content with high precision over a year or more after training and (2) simple terms and locations are the most informative feature classes (despite training on classes labeled via hashtags). &lt;/p&gt; }, 
number={1}, 
journal={Proceedings of the International AAAI Conference on Web and Social Media}, 
author={Iman, Zahra and Sanner, Scott and Bouadjenek, Mohamed Reda and Xie, Lexing}, 
year={2017}, 
month={May}, 
pages={552-555},
pdf={ICWSM2017.pdf},
selected={false},
preview={aaai-logo.png},
}


@article{10.1093/database/bax021,
    author = {Bouadjenek, Mohamed Reda and Verspoor, Karin and Zobel, Justin},
    title = {Literature consistency of bioinformatics sequence databases is effective for assessing record quality},
    journal = {Database},
    volume = {2017},
    pages = {bax021},
    year = {2017},
    month = {03},
    abstract = {Bioinformatics sequence databases such as Genbank or UniProt contain hundreds of millions of records of genomic data. These records are derived from direct submissions from individual laboratories, as well as from bulk submissions from large-scale sequencing centres; their diversity and scale means that they suffer from a range of data quality issues including errors, discrepancies, redundancies, ambiguities, incompleteness and inconsistencies with the published literature. In this work, we seek to investigate and analyze the data quality of sequence databases from the perspective of a curator, who must detect anomalous and suspicious records. Specifically, we emphasize the detection of inconsistent records with respect to the literature. Focusing on GenBank, we propose a set of 24 quality indicators, which are based on treating a record as a query into the published literature, and then use query quality predictors. We then carry out an analysis that shows that the proposed quality indicators and the quality of the records have a mutual relationship, in which one depends on the other. We propose to represent record-literature consistency as a vector of these quality indicators. By reducing the dimensionality of this representation for visualization purposes using principal component analysis, we show that records which have been reported as inconsistent with the literature fall roughly in the same area, and therefore share similar characteristics. By manually analyzing records not previously known to be erroneous that fall in the same area than records know to be inconsistent, we show that one record out of four is inconsistent with respect to the literature. This high density of inconsistent record opens the way towards the development of automatic methods for the detection of faulty records. We conclude that literature inconsistency is a meaningful strategy for identifying suspicious records.Database URL: https://github.com/rbouadjenek/DQBioinformatics},
    issn = {1758-0463},
    doi = {10.1093/database/bax021},
    url = {https://doi.org/10.1093/database/bax021},
    eprint = {https://academic.oup.com/database/article-pdf/doi/10.1093/database/bax021/19232099/bax021.pdf},
	pdf={bax021.pdf},
	selected={false},
	preview={oxford_university_press.jpg},
}








