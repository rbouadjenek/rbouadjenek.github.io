---
---
@inproceedings{10.1145/1869790.1869884,
author = {Lassoued, Yassine and Bouadjenek, Mohamed Reda and Boucelma, Omar and Lemos, Fernando and Bouzeghoub, Mokrane},
title = {GQBox: geospatial data quality assessment},
year = {2010},
isbn = {9781450304283},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1869790.1869884},
doi = {10.1145/1869790.1869884},
abstract = {In order to measure and assess the quality of GIS, there exist a sparse offer of tools, providing specific functions with their own interest but are not sufficient to deal with broader user's requirements. Interoperability of these tools remains a technical challenge because of the heterogeneity of their models and access patterns. On the other side, quality analysts require more and more integration facilities that allow them to consolidate and aggregate multiple quality measures acquired from different observations or data sources, in using/combining seamlessly different quality tools. Clearly, there is a gap between users's requirements and the spatial data quality market. This demo paper will illustrate GQBox, a geographic quality (tool)box. GQBox supplies a standards-based generic meta model that supports the definition of quality goals and metrics, and it provides a service-based infrastructure that allows interoperability among several quality tools.},
booktitle = {Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {534–535},
numpages = {2},
keywords = {web services, geospatial data quality},
location = {San Jose, California},
series = {GIS '10},
pdf={p534-lassoued.pdf},
selected={false},
preview={acm.png},
abbr={GIS},
}

@inproceedings{10.1145/2009916.2010075,
author = {Bouadjenek, Mohamed Reda and Hacid, Hakim and Bouzeghoub, Mokrane and Daigremont, Johann},
title = {Personalized social query expansion using social bookmarking systems},
year = {2011},
isbn = {9781450307574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2009916.2010075},
doi = {10.1145/2009916.2010075},
abstract = {We propose a new approach for social and personalized query expansion using social structures in the Web 2.0. While focusing on social tagging systems, the proposed approach considers (i) the semantic similarity between tags composing a query, (ii) a social proximity between the query and the user profile, and (iii) on the fly, a strategy for expanding user queries. The proposed approach has been evaluated using a large dataset crawled from del.icio.us.},
booktitle = {Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1113–1114},
numpages = {2},
keywords = {social networks, social information retrieval, personalization},
location = {Beijing, China},
series = {SIGIR '11},
pdf={p1113-bouadjenek.pdf},
selected={false},
preview={acm.png},
abbr={SIGIR},
}

@InProceedings{10.1007/978-3-642-39200-9_24,
author={Bouadjenek, Mohamed Reda
and Bennamane, Amyn
and Hacid, Hakim
and Bouzeghoub, Mokrane},
editor={Daniel, Florian
and Dolog, Peter
and Li, Qing},
title={Evaluation of Personalized Social Ranking Functions of Information Retrieval},
booktitle={Web Engineering},
year={2013},
publisher={Springer Berlin Heidelberg},
address={Berlin, Heidelberg},
pages={283--290},
abstract={There is currently a number of interesting research works performed in the area of bridging the gap between Social Networks and Information Retrieval (IR). This is mainly done by enhancing the IR process with social information. Hence, many approaches have been proposed to improve the ranking process by personalizing it using social features. In this paper, we review some of these ranking functions.},
isbn={978-3-642-39200-9},
pdf={ICWE2013.pdf},
selected={false},
preview={springer.png},
abbr={ICWE},
}

@inproceedings{10.1145/2484028.2484131,
author = {Bouadjenek, Mohamed Reda and Hacid, Hakim and Bouzeghoub, Mokrane},
title = {Sopra: a new social personalized ranking function for improving web search},
year = {2013},
isbn = {9781450320344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484028.2484131},
doi = {10.1145/2484028.2484131},
abstract = {We present in this paper a contribution to IR modeling by proposing a new ranking function called SoPRa that considers the social dimension of the Web. This social dimension is any social information that surrounds documents along with the social context of users. Currently, our approach relies on folksonomies for extracting these social contexts, but it can be extended to use any social meta-data, e.g. comments, ratings, tweets, etc. The evaluation performed on our approach shows its benefits for personalized search.},
booktitle = {Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {861–864},
numpages = {4},
keywords = {information retrieval, social networks},
location = {Dublin, Ireland},
series = {SIGIR '13},
pdf={sp086-bouadjenek.pdf},
selected={true},
preview={acm.png},
abbr={SIGIR},
}

@inproceedings{10.1145/2484028.2484130,
author = {BOUADJENEK, Mohamed Reda and Hacid, Hakim and Bouzeghoub, Mokrane and Vakali, Athena},
title = {Using social annotations to enhance document representation for personalized search},
year = {2013},
isbn = {9781450320344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484028.2484130},
doi = {10.1145/2484028.2484130},
abstract = {In this paper, we present a contribution to IR modeling. We propose an approach that computes on the fly, a Personalized Social Document Representation (PSDR) of each document per user based on his social activities. The PSDRs are used to rank documents with respect to a query. This approach has been intensively evaluated on a large public dataset, showing significant benefits for personalized search.},
booktitle = {Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1049–1052},
numpages = {4},
keywords = {information retrieval, social networks},
location = {Dublin, Ireland},
series = {SIGIR '13},
pdf={sp085-bouadjenek.pdf},
selected={false},
preview={acm.png},
abbr={SIGIR},
}

@inproceedings{10.1145/2487575.2487705,
author = {Bouadjenek, Mohamed Reda and Hacid, Hakim and Bouzeghoub, Mokrane},
title = {LAICOS: an open source platform for personalized social web search},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2487705},
doi = {10.1145/2487575.2487705},
abstract = {In this paper, we introduce LAICOS, a social Web search engine as a contribution to the growing area of Social Information Retrieval (SIR). Social information and personalization are at the heart of LAICOS. On the one hand, the social context of documents is added as a layer to their textual content traditionally used for indexing to provide Personalized Social Document Representations. On the other hand, the social context of users is used for the query expansion process using the Personalized Social Query Expansion framework (PSQE) proposed in our earlier works. We describe the different components of the system while relying on social bookmarking systems as a source of social information for personalizing and enhancing the IR process. We show how the internal structure of indexes as well as the query expansion process operated using social information.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1446–1449},
numpages = {4},
keywords = {social networks, social information retrieval, personalization, information retrieval},
location = {Chicago, Illinois, USA},
series = {KDD '13},
pdf={demo16-bouadjenek.pdf},
selected={false},
preview={acm.png},
abbr={KDD},
}

@inproceedings{10.1145/2746090.2746092,
author = {Bouadjenek, Mohamed Reda and Sanner, Scott and Ferraro, Gabriela},
title = {A study of query reformulation for patent prior art search with partial patent applications},
year = {2015},
isbn = {9781450335225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2746090.2746092},
doi = {10.1145/2746090.2746092},
abstract = {Patents are used by legal entities to legally protect their inventions and represent a multi-billion dollar industry of licensing and litigation. In 2014, 326,033 patent applications were approved in the US alone -- a number that has doubled in the past 15 years and which makes prior art search a daunting, but necessary task in the patent application process. In this work, we seek to investigate the efficacy of prior art search strategies from the perspective of the inventor who wishes to assess the patentability of their ideas prior to writing a full application. While much of the literature inspired by the evaluation framework of the CLEF-IP competition has aimed to assist patent examiners in assessing prior art for complete patent applications, less of this work has focused on patent search with queries representing partial applications. In the (partial) patent search setting, a query is often much longer than in other standard IR tasks, e.g., the description section may contain hundreds or even thousands of words. While the length of such queries may suggest query reduction strategies to remove irrelevant terms, intentional obfuscation and general language used in patents suggests that it may help to expand queries with additionally relevant terms. To assess the trade-offs among all of these pre-application prior art search strategies, we comparatively evaluate a variety of partial application search and query reformulation methods. Among numerous findings, querying with a full description, perhaps in conjunction with generic (non-patent specific) query reduction methods, is recommended for best performance. However, we also find that querying with an abstract represents the best trade-off in terms of writing effort vs. retrieval efficacy (i.e., querying with the description sections only lead to marginal improvements) and that for such relatively short queries, generic query expansion methods help.},
booktitle = {Proceedings of the 15th International Conference on Artificial Intelligence and Law},
pages = {23–32},
numpages = {10},
keywords = {patent search, query reformulation},
location = {San Diego, California},
series = {ICAIL '15},
pdf={ICAIL2015.pdf},
selected={false},
preview={acm.png},
abbr={ICAIL},
}

@inproceedings{10.1145/2766462.2767801,
author = {Golestan Far, Mona and Sanner, Scott and Bouadjenek, Mohamed Reda and Ferraro, Gabriela and Hawking, David},
title = {On Term Selection Techniques for Patent Prior Art Search},
year = {2015},
isbn = {9781450336215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2766462.2767801},
doi = {10.1145/2766462.2767801},
abstract = {In this paper, we investigate the influence of term selection on retrieval performance on the CLEF-IP prior art test collection, using the Description section of the patent query with Language Model (LM) and BM25 scoring functions. We find that an oracular relevance feedback system that extracts terms from the judged relevant documents far outperforms the baseline and performs twice as well on MAP as the best competitor in CLEF-IP 2010. We find a very clear term selection value threshold for use when choosing terms. We also noticed that most of the useful feedback terms are actually present in the original query and hypothesized that the baseline system could be substantially improved by removing negative query terms. We tried four simple automated approaches to identify negative terms for query reduction but we were unable to notably improve on the baseline performance with any of them. However, we show that a simple, minimal interactive relevance feedback approach where terms are selected from only the first retrieved relevant document outperforms the best result from CLEF-IP 2010 suggesting the promise of interactive methods for term selection in patent prior art search.},
booktitle = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {803–806},
numpages = {4},
keywords = {patent search, query reformulation},
location = {Santiago, Chile},
series = {SIGIR '15},
pdf={p803-golestan-far.pdf},
selected={false},
preview={acm.png},
abbr={SIGIR},
}

@article{BOUADJENEK20161,
title = {Social networks and information retrieval, how are they converging? A survey, a taxonomy and an analysis of social information retrieval approaches and platforms},
journal = {Information Systems},
volume = {56},
pages = {1-18},
year = {2016},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2015.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S030643791500160X},
author = {Mohamed Reda Bouadjenek and Hakim Hacid and Mokrane Bouzeghoub},
keywords = {Information Retrieval, Social networks, Social Information Retrieval, Social search, Social recommendation},
abstract = {There is currently a number of research work performed in the area of bridging the gap between Information Retrieval (IR) and Online Social Networks (OSN). This is mainly done by enhancing the IR process with information coming from social networks, a process called Social Information Retrieval (SIR). The main question one might ask is What would be the benefits of using social information (no matter whether it is content or structure) into the information retrieval process and how is this currently done? With the growing number of efforts towards the combination of IR and social networks, it is necessary to build a clearer picture of the domain and synthesize the efforts in a structured and meaningful way. This paper reviews different efforts in this domain. It intends to provide a clear understanding of the issues as well as a clear structure of the contributions. More precisely, we propose (i) to review some of the most important contributions in this domain to understand the principles of SIR, (ii) a taxonomy to categorize these contributions, and finally, (iii) an analysis of some of these contributions and tools with respect to several criteria, which we believe are crucial to design an effective SIR approach. This paper is expected to serve researchers and practitioners as a reference to help them structuring the domain, position themselves and, ultimately, help them to propose new contributions or improve existing ones.},
pdf={IS-D-14-351.pdf},
selected={true},
preview={Elsevier.svg.png},
abbr={InfoSci},
}

@article{BOUADJENEK2016614,
title = {PerSaDoR: Personalized social document representation for improving web search},
journal = {Information Sciences},
volume = {369},
pages = {614-633},
year = {2016},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2016.07.046},
url = {https://www.sciencedirect.com/science/article/pii/S0020025516305278},
author = {Mohamed Reda Bouadjenek and Hakim Hacid and Mokrane Bouzeghoub and Athena Vakali},
keywords = {Information retrieval, Social networks, Social information retrieval, Social search, Social recommendation},
abstract = {In this paper, we discuss a contribution towards the integration of social information in the index structure of an IR system. Since each user has his/her own understanding and point of view of a given document, we propose an approach in which the index model provides a Personalized Social Document Representation (PerSaDoR) of each document per user based on his/her activities in a social tagging system. The proposed approach relies on matrix factorization to compute the PerSaDoR of documents that match a query, at query time. The complexity analysis shows that our approach scales linearly with the number of documents that match the query, and thus, it can scale to very large datasets. PerSaDoR has been also intensively evaluated by an offline study and by a user survey operated on a large public dataset from delicious showing significant benefits for personalized search compared to state of the art methods.},
pdf={INS-D-15-2480.pdf},
selected={false},
preview={Elsevier.svg.png},
abbr={InfoSys},
}

@article{10.1093/database/bax062,
    author = {Bouadjenek, Mohamed Reda and Verspoor, Karin},
    title = {Multi-field query expansion is effective for biomedical dataset retrieval},
    journal = {Database},
    volume = {2017},
    pages = {bax062},
    year = {2017},
    month = {09},
    abstract = {In the context of the bioCADDIE challenge addressing information retrieval of biomedical datasets, we propose a method for retrieval of biomedical data sets with heterogenous schemas through query reformulation. In particular, the method proposed transforms the initial query into a multi-field query that is then enriched with terms that are likely to occur in the relevant datasets. We compare and evaluate two query expansion strategies, one based on the Rocchio method and another based on a biomedical lexicon. We then perform a comprehensive comparative evaluation of our method on the bioCADDIE dataset collection for biomedical retrieval. We demonstrate the effectiveness of our multi-field query method compared to two baselines, with MAP improved from 0.2171 and 0.2669 to 0.2996. We also show the benefits of query expansion, where the Rocchio expanstion method improves the MAP for our two baselines from 0.2171 and 0.2669 to 0.335. We show that the Rocchio query expansion method slightly outperforms the one based on the biomedical lexicon as a source of terms, with an improvement of roughly 3\% for MAP. However, the query expansion method based on the biomedical lexicon is much less resource intensive since it does not require computation of any relevance feedback set or any initial execution of the query. Hence, in term of trade-off between efficiency, execution time and retrieval accuracy, we argue that the query expansion method based on the biomedical lexicon offers the best performance for a prototype biomedical data search engine intended to be used at a large scale. In the official bioCADDIE challenge results, although our approach is ranked seventh in terms of the infNDCG evaluation metric, it ranks second in term of P@10 and NDCG. Hence, the method proposed here provides overall good retrieval performance in relation to the approaches of other competitors. Consequently, the observations made in this paper should benefit the development of a Data Discovery Index prototype or the improvement of the existing one.},
    issn = {1758-0463},
    doi = {10.1093/database/bax062},
    url = {https://doi.org/10.1093/database/bax062},
    eprint = {https://academic.oup.com/database/article-pdf/doi/10.1093/database/bax062/22892713/bax062.pdf},
	pdf={bax062.pdf},
	selected={false},
	preview={oxford_university_press.jpg},
}

@article{BOUADJENEK2017229,
title = {Automated detection of records in biological sequence databases that are inconsistent with the literature},
journal = {Journal of Biomedical Informatics},
volume = {71},
pages = {229-240},
year = {2017},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2017.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S1532046417301399},
author = {Mohamed Reda Bouadjenek and Karin Verspoor and Justin Zobel},
keywords = {Data analysis, Data quality, Bioinformatics databases, Anomaly detection},
abstract = {We investigate and analyse the data quality of nucleotide sequence databases with the objective of automatic detection of data anomalies and suspicious records. Specifically, we demonstrate that the published literature associated with each data record can be used to automatically evaluate its quality, by cross-checking the consistency of the key content of the database record with the referenced publications. Focusing on GenBank, we describe a set of quality indicators based on the relevance paradigm of information retrieval (IR). Then, we use these quality indicators to train an anomaly detection algorithm to classify records as “confident” or “suspicious”. Our experiments on the PubMed Central collection show assessing the coherence between the literature and database records, through our algorithms, is an effective mechanism for assisting curators to perform data cleansing. Although fewer than 0.25% of the records in our data set are known to be faulty, we would expect that there are many more in GenBank that have not yet been identified. By automated comparison with literature they can be identified with a precision of up to 10% and a recall of up to 30%, while strongly outperforming several baselines. While these results leave substantial room for improvement, they reflect both the very imbalanced nature of the data, and the limited explicitly labelled data that is available. Overall, the obtained results show promise for the development of a new kind of approach to detecting low-quality and suspicious sequence records based on literature analysis and consistency. From a practical point of view, this will greatly help curators in identifying inconsistent records in large-scale sequence databases by highlighting records that are likely to be inconsistent with the literature.},
pdf={JBI-17-134.pdf},
selected={false},
preview={Elsevier.svg.png},
abbr={JBI},
}

@inproceedings{10.1145/3132847.3133051,
author = {Bouadjenek, Mohamed Reda and Verspoor, Karin and Zobel, Justin},
title = {Learning Biological Sequence Types Using the Literature},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133051},
doi = {10.1145/3132847.3133051},
abstract = {We explore in this paper automatic biological sequence type classification for records in biological sequence databases. The sequence type attribute provides important information about the nature of a sequence represented in a record, and is often used in search to filter out irrelevant sequences. However, the sequence type attribute is generally a non-mandatory free-text field, and thus it is subject to many errors including typos, mis-assignment, and non-assignment. In GenBank, this problem concerns roughly 18\% of records, an alarming number that should worry the biocuration community. To address this problem of automatic sequence type classification, we propose the use of literature associated to sequence records as an external source of knowledge that can be leveraged for the classification task. We define a set of literature-based features and train a machine learning algorithm to classify a record into one of six primary sequence types. The main intuition behind using the literature for this task is that sequences appear to be discussed differently in scientific articles, depending on their type. The experiments we have conducted on the PubMed Central collection show that the literature is indeed an effective way to address this problem of sequence type classification. Our classification method reached an accuracy of 92.7\%, and substantially outperformed two baseline approaches used for comparison.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1991–1994},
numpages = {4},
keywords = {data quality, data cleansing, data analysis, biological databases},
location = {Singapore, Singapore},
series = {CIKM '17},
pdf={sp0228-bouadjenek.pdf},
selected={false},
preview={acm.png},
abbr={CIKM},
}

@article{Iman_Sanner_Bouadjenek_Xie_2017, 
title={A Longitudinal Study of Topic Classification on Twitter}, 
volume={11}, 
url={https://ojs.aaai.org/index.php/ICWSM/article/view/14934}, 
DOI={10.1609/icwsm.v11i1.14934}, 
abstractNote={ &lt;p&gt; Twitter represents a massively distributed information source over a kaleidoscope of topics ranging from social and political events to entertainment and sports news. While recent work has suggested that variations on standard classifiers can be effectively trained as topical filters (Lin, Snow, and Morgan 2011; Yang et al. 2014; Magdy and Elsayed 2014), there remain many open questions about the efficacy of such classification-based filtering approaches. For example, over a year or more after training, how well do such classifiers generalize to future novel topical content, and are such results stable across a range of topics? Furthermore, what features and feature classes are most critical for long-term classifier performance? To answer these questions, we collected a corpus of over 800 million English Tweets via the Twitter streaming API during 2013 and 2014 and learned topic classifiers for 10 diverse themes ranging from social issues to celebrity deaths to the “Iran nuclear deal”. The results of this long-term study of topic classifier performance provide a number of important insights, among them that (1) such classifiers can indeed generalize to novel topical content with high precision over a year or more after training and (2) simple terms and locations are the most informative feature classes (despite training on classes labeled via hashtags). &lt;/p&gt; }, 
number={1}, 
journal={Proceedings of the International AAAI Conference on Web and Social Media}, 
author={Iman, Zahra and Sanner, Scott and Bouadjenek, Mohamed Reda and Xie, Lexing}, 
year={2017}, 
month={May}, 
pages={552-555},
pdf={ICWSM2017.pdf},
selected={false},
preview={aaai-logo.png},
abbr={ICWSM},

}


@article{10.1093/database/bax021,
    author = {Bouadjenek, Mohamed Reda and Verspoor, Karin and Zobel, Justin},
    title = {Literature consistency of bioinformatics sequence databases is effective for assessing record quality},
    journal = {Database},
    volume = {2017},
    pages = {bax021},
    year = {2017},
    month = {03},
    abstract = {Bioinformatics sequence databases such as Genbank or UniProt contain hundreds of millions of records of genomic data. These records are derived from direct submissions from individual laboratories, as well as from bulk submissions from large-scale sequencing centres; their diversity and scale means that they suffer from a range of data quality issues including errors, discrepancies, redundancies, ambiguities, incompleteness and inconsistencies with the published literature. In this work, we seek to investigate and analyze the data quality of sequence databases from the perspective of a curator, who must detect anomalous and suspicious records. Specifically, we emphasize the detection of inconsistent records with respect to the literature. Focusing on GenBank, we propose a set of 24 quality indicators, which are based on treating a record as a query into the published literature, and then use query quality predictors. We then carry out an analysis that shows that the proposed quality indicators and the quality of the records have a mutual relationship, in which one depends on the other. We propose to represent record-literature consistency as a vector of these quality indicators. By reducing the dimensionality of this representation for visualization purposes using principal component analysis, we show that records which have been reported as inconsistent with the literature fall roughly in the same area, and therefore share similar characteristics. By manually analyzing records not previously known to be erroneous that fall in the same area than records know to be inconsistent, we show that one record out of four is inconsistent with respect to the literature. This high density of inconsistent record opens the way towards the development of automatic methods for the detection of faulty records. We conclude that literature inconsistency is a meaningful strategy for identifying suspicious records.Database URL: https://github.com/rbouadjenek/DQBioinformatics},
    issn = {1758-0463},
    doi = {10.1093/database/bax021},
    url = {https://doi.org/10.1093/database/bax021},
    eprint = {https://academic.oup.com/database/article-pdf/doi/10.1093/database/bax021/19232099/bax021.pdf},
	pdf={bax021.pdf},
	selected={false},
	preview={oxford_university_press.jpg},
}

@inproceedings{bouadjenek2018dbkda,
  author       = {Mohamed Reda Bouadjenek and Esther Pacitti and Maximilien Servajean and Florent Masseglia and Amr El Abbadi},
  title        = {A Distributed Collaborative Filtering Algorithm Using Multiple Data Sources},
  booktitle    = {The Tenth International Conference on Advances in Databases, Knowledge, and Data Applications},
  series       = {DBKDA '18},
  year         = {2018},
  location     = {Nice, France},
  numpages     = {10},
  publisher    = {International Academy, Research, and Industry Association},
  abstract     = {Collaborative Filtering (CF) is one of the most commonly used recommendation methods. CF consists in predicting whether, or how much, a user will like (or dislike) an item by leveraging the knowledge of the user’s preferences as well as that of other users. In practice, users interact and express their opinion on only a small subset of items, which makes the corresponding user-item rating matrix very sparse. Such data sparsity yields two main problems for recommender systems: (1) the lack of data to effectively model users’ preferences, and (2) the lack of data to effectively model item characteristics. However, there are often many other data sources that are available to a recommender system provider, which can describe user interests and item characteristics (e.g., users’ social network, tags associated to items, etc.). These valuable data sources may supply useful information to enhance a recommendation system in modeling users’ preferences and item characteristics more accurately and thus, hopefully, to make recommenders more precise. For various reasons, these data sources may be managed by clusters of different data centers, thus requiring the development of distributed solutions. In this paper, we propose a new distributed collaborative filtering algorithm, which exploits and combines multiple and diverse data sources to improve recommendation quality. Our experimental evaluation using real datasets shows the effectiveness of our algorithm compared to state-of-the-art recommendation algorithms.},
  
  award        = {Best Paper Award},
  certificate  = {assets/pdf/dbkda2018_a2.pdf},
  pdf          = {DBKDA2018_v2.0.pdf},
  preview      = {iaria.png},
  selected     = {false},
  abbr         = {DBKDA},
}

@Inbook{Bouadjenek2019,
author={Bouadjenek, Mohamed Reda
and Hacid, Hakim
and Bouzeghoub, Mokrane},
editor={Hameurlain, Abdelkader
and Wagner, Roland
and Morvan, Franck
and Tamine, Lynda},
title={Personalized Social Query Expansion Using Social Annotations},
bookTitle={Transactions on Large-Scale Data- and Knowledge-Centered Systems XL},
year={2019},
publisher={Springer Berlin Heidelberg},
address={Berlin, Heidelberg},
pages={1--25},
abstract={Query expansion is a query pre-processing technique that adds to a given query, terms that are likely to occur in relevant documents in order to improve information retrieval accuracy. A key problem to solve is ``how to identify the terms to be added to a query?'' While considering social tagging systems as a data source, we propose an approach that selects terms based on (i) the semantic similarity between tags composing a query, (ii) a social proximity between the query and the user for a personalized expansion, and (iii) a strategy for expanding, on the fly, user queries. We demonstrate the effectiveness of our approach by an intensive evaluation on three large public datasets crawled from delicious, Flickr, and CiteULike. We show that the expanded queries built by our method provide more accurate results as compared to the initial queries, by increasing the MAP in a range of 10 to 16{\%} on the three datasets. We also compare our method to three state of the art baselines, and we show that our query expansion method allows significant improvement in the MAP, with a boost in a range between 5 to 18{\%}.},
isbn={978-3-662-58664-8},
doi={10.1007/978-3-662-58664-8_1},
url={https://doi.org/10.1007/978-3-662-58664-8_1},
pdf= {TLDKS-12008.pdf},
preview= {springer.png},
selected= {false},
}

@inproceedings{10.1145/3295750.3298914,
author = {Bouadjenek, Mohamed Reda and Sanner, Scott},
title = {Relevance-driven Clustering for Visual Information Retrieval on Twitter},
year = {2019},
isbn = {9781450360258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295750.3298914},
doi = {10.1145/3295750.3298914},
abstract = {Geo-temporal visualization of Twitter search results is a challenging task since the simultaneous display of all matching tweets would result in a saturated and unreadable display. In such settings, clustering search results can assist users to scan only a few coherent groups of related tweets rather than many individual tweets. However, in practice, the use of unsupervised clustering methods such as K -Means does not necessarily guarantee that the clusters themselves are relevant. Therefore, we develop a novel method of relevance-driven clustering for visual information retrieval to supply users with highly relevant clusters representing different information perspectives of their queries. We specifically propose a Visual Twitter Information Retrieval (Viz-TIR) tool for relevance-driven clustering and ranking of Twitter search results. At the heart of Viz-TIR is a fast greedy algorithm that optimizes an approximation of an expected F1-Score metric to generate these clusters. We demonstrate its effectiveness w.r.t. K -Means and a baseline method that shows all top matching results on a scenario related to searching natural disasters in US-based Twitter data spanning 2013 and 2014. Our demo shows that Viz-TIR is easy to use and more precise in extracting geo-temporally coherent clusters given search queries in comparison to K-Means, thus aiding the user in visually searching and browsing social network content. Overall, we believe this work enables new opportunities for the synthesis of information retrieval as well as combined relevance and display-aware optimization techniques to support query-adaptive visual information exploration interfaces.},
booktitle = {Proceedings of the 2019 Conference on Human Information Interaction and Retrieval},
pages = {349–353},
numpages = {5},
keywords = {relevance-driven clustering, visual search interfaces},
location = {Glasgow, Scotland UK},
series = {CHIIR '19},
pdf= {p349-bouadjenek.pdf},
preview= {acm.png},
selected= {false},
abbr= {CHIIR},
}

@article{Bouadjenek2019,
  author       = {Mohamed Reda Bouadjenek and Justin Zobel and Karin Verspoor},
  title        = {Automated assessment of biological database assertions using the scientific literature},
  journal      = {BMC Bioinformatics},
  year         = {2019},
  volume       = {20},
  number       = {1},
  pages        = {216},
  abstract     = {The large biological databases such as GenBank contain vast numbers of records, the content of which is substantively based on external resources, including published literature. Manual curation is used to establish whether the literature and the records are indeed consistent. We explore in this paper an automated method for assessing the consistency of biological assertions, to assist biocurators, which we call BARC, Biocuration tool for Assessment of Relation Consistency. In this method a biological assertion is represented as a relation between two objects (for example, a gene and a disease); we then use our novel set-based relevance algorithm SaBRA to retrieve pertinent literature, and apply a classifier to estimate the likelihood that this relation (assertion) is correct.},
  doi          = {10.1186/s12859-019-2801-x},
  url          = {https://doi.org/10.1186/s12859-019-2801-x},
  issn         = {1471-2105},
pdf= {BINF-D-18-00977.pdf},
preview= {bmc.png},
selected= {false},
}


@inbook{doi:10.1137/1.9781611975673.25,
author = {Yakun Wang and Ga Wu and Mohamed Reda Bouadjenek and Scott Sanner and Sen Su and Zhongbao Zhang},
title = {A Novel Regularizer for Temporally Stable Learning with an Application to Twitter Topic Classification},
booktitle = {Proceedings of the 2019 SIAM International Conference on Data Mining (SDM)},
chapter = {},
pages = {217-225},
doi = {10.1137/1.9781611975673.25},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611975673.25},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611975673.25},
    abstract = { Abstract Supervised topic classifiers for Twitter and other media sources are important in a variety of long-term topic tracking tasks. Unfortunately, over long periods of time, features that are predictive during the training period may prove ephemeral and fail to generalize to prediction at future times. For example, if we trained a classifier to identify tweets concerning the topic of “Celebrity Death”, individual celebrity names and terms associated with these celebrities such as “Nelson Mandela” or “South Africa” would prove to be temporally unstable since they would not generalize over long periods of time; in contrast, terms like “RIP” (rest in peace) would prove to be temporally stable predictors of this topic over long periods of time. In this paper, we aim to design supervised learning methods for Twitter topic classifiers that are capable of automatically downweighting temporally unstable features to improve future generalization. To do this, we first begin with an oracular approach that chooses temporally stable features based on knowledge of both train and test data labels. We then search for feature metrics evaluated on only the training data that are capable of recovering the temporally stable features identified by our oracular definition. We next embed the top-performing metric as a temporal stability regularizer in logistic regression with the important property that the overall training objective retains convexity, hence enabling a globally optimal solution. Finally, we train our topic classifiers on 6 Twitter topics over roughly one year of data and evaluate on the following year of data, showing that logistic regression with our temporal stability regularizer generally outperforms logistic regression without such regularization across the full precision-recall continuum. Overall, these results establish a novel regularizer for training long-term temporally stable topic classifiers for Twitter and beyond. },
pdf= {SDM2019.pdf},
preview= {SIAM_logo.png},
selected= {false},
abbr= {SDM},
}

@inproceedings{10.1145/3331184.3331292,
author = {Wu, Ga and Bouadjenek, Mohamed Reda and Sanner, Scott},
title = {One-Class Collaborative Filtering with the Queryable Variational Autoencoder},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331292},
doi = {10.1145/3331184.3331292},
abstract = {Variational Autoencoder (VAE) based methods for Collaborative Filtering (CF) demonstrate remarkable performance for one-class (implicit negative) recommendation tasks by extending autoencoders with relaxed but tractable latent distributions. Explicitly modeling a latent distribution over user preferences allows VAEs to learn user and item representations that not only reproduce observed interactions, but also generalize them by leveraging learning from similar users and items. Unfortunately, VAE-CF can exhibit suboptimal learning properties; e.g., VAE-CFs will increase their prediction confidence as they receive more preferences per user, even when those preferences may vary widely and create ambiguity in the user representation. To address this issue, we propose a novel Queryable Variational Autoencoder (Q-VAE) variant of the VAE that explicitly models arbitrary conditional relationships between observations. The proposed model appropriately increases uncertainty (rather than reduces it) in cases where a large number of user preferences may lead to an ambiguous user representation. Our experiments on two benchmark datasets show that the Q-VAE generally performs comparably or outperforms VAE-based recommenders as well as other state-of-the-art approaches and is generally competitive across the user preference density spectrum, where other methods peak for certain preference density levels.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {921–924},
numpages = {4},
keywords = {variational autoencoder, one-class collaborative filtering, conditional inference},
location = {Paris, France},
series = {SIGIR'19},
pdf= {SIGIR_Short_2019.pdf},
preview= {acm.png},
selected= {false},
abbr= {SIGIR},
}

@article{GUPTA2020878,
title = {Evaluation of Machine Learning Algorithms for Predicting Readmission After Acute Myocardial Infarction Using Routinely Collected Clinical Data},
journal = {Canadian Journal of Cardiology},
volume = {36},
number = {6},
pages = {878-885},
year = {2020},
issn = {0828-282X},
doi = {https://doi.org/10.1016/j.cjca.2019.10.023},
url = {https://www.sciencedirect.com/science/article/pii/S0828282X19313583},
author = {Shagun Gupta and Dennis T. Ko and Paymon Azizi and Mohamed Reda Bouadjenek and Maria Koh and Alice Chong and Peter C. Austin and Scott Sanner},
abstract = {Background
The ability to predict readmission accurately after hospitalization for acute myocardial infarction (AMI) is limited in current statistical models. Machine-learning (ML) methods have shown improved predictive ability in various clinical contexts, but their utility in predicting readmission after hospitalization for AMI is unknown.
Methods
Using detailed clinical information collected from patients hospitalized with AMI, we evaluated 6 ML algorithms (logistic regression, naïve Bayes, support vector machines, random forest, gradient boosting, and deep neural networks) to predict readmission within 30 days and 1 year of discharge. A nested cross-validation approach was used to develop and test models. We used C-statistics to compare discriminatory capacity, whereas the Brier score was used to indicate overall model performance. Model calibration was assessed using calibration plots.
Results
The 30-day readmission rate was 16.3%, whereas the 1-year readmission rate was 45.1%. For 30-day readmission, the discriminative ability for the ML models was modest (C-statistic 0.641; 95% confidence interval (CI), 0.621-0.662 for gradient boosting) and did not outperform previously reported methods. For 1-year readmission, different ML models showed moderate performance, with C-statistics around 0.72. Despite modest discriminatory capabilities, the observed readmission rates were markedly higher in the tenth decile of predicted risk compared with the first decile of predicted risk for both 30-day and 1-year readmission.
Conclusions
Despite including detailed clinical information and evaluating various ML methods, these models did not have better discriminatory ability to predict readmission outcomes compared with previously reported methods.
Résumé
Contexte
Les modèles statistiques actuels ne permettent pas de prédire avec exactitude la réadmission après une hospitalisation pour cause d’infarctus aigu du myocarde (IAM). Les méthodes de prédiction faisant appel à l’apprentissage automatique ont été associées à une amélioration de la capacité de prédiction dans divers contextes cliniques, mais leur utilité pour prédire la réadmission après une hospitalisation pour cause d’IAM demeure inconnue.
Méthodologie
À l’aide de données cliniques détaillées recueillies auprès de patients hospitalisés pour un IAM, nous avons évalué six algorithmes d’apprentissage automatique (régression logistique, classification naïve bayésienne, machine à vecteurs de support, forêt aléatoire, boosting par descente de gradient fonctionnelle et réseaux neuronaux d’apprentissage profond) pour prédire la réadmission dans les 30 jours et dans l’année suivant la sortie de l’hôpital. Les modèles ont été mis au point et testés à l’aide d’une approche de validation croisée imbriquée. Nous avons utilisé la statistique C pour comparer la capacité de discrimination des différents modèles, et le score de Brier pour en chiffrer le rendement global. Le calage des modèles a été évalué au moyen de courbes d’étalonnage.
Résultats
Le taux de réadmission à 30 jours était de 16,3 %, tandis que le taux de réadmission à 1 an était de 45,1 %. Dans le cas de la réadmission à 30 jours, la capacité de discrimination des modèles d’apprentissage automatique était modeste (statistique C : 0,641; intervalle de confiance [IC] à 95 % : 0,621-0,662 pour le boosting par descente de gradient fonctionnelle) et n’était pas supérieure à celle des méthodes déjà utilisées. Dans le cas de la réadmission à 1 an, différents modèles d’apprentissage automatique se sont révélés modérément efficaces, la statistique C se chiffrant à environ 0,72. En dépit des modestes capacités de discrimination des différentes méthodes, les taux de réadmission observés étaient nettement plus élevés dans le dixième décile du risque prédit comparativement à ceux du premier décile, pour la réadmission à 30 jours comme pour la réadmission à 1 an.
Conclusions
Malgré le recours à des données cliniques détaillées et à différentes méthodes d’apprentissage automatique, les modèles évalués n’ont pas montré une capacité de discrimination supérieure à celle des méthodes déjà utilisées pour prédire la réadmission.}
,
pdf= {CJC2020.pdf},
preview={Elsevier.svg.png},
selected= {false},
}


@ARTICLE{8827945,
  author={Zhang, Bohan and Sanner, Scott and Bouadjenek, Mohamed Reda and Gupta, Shagun},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Bayesian Networks for Data Integration in the Absence of Foreign Keys}, 
  year={2020},
  volume={32},
  number={4},
  pages={803-808},
  keywords={Bayes methods;Probabilistic logic;Data integration;Periodic structures;Education;Data models;Random variables;Bayesian networks;probabilistic data integration},
  doi={10.1109/TKDE.2019.2940019},
pdf= {TKDE2940019.pdf},
preview= {IEEELogo.png},
selected= {false},
abbr= {TKDE},
}

@article{BOUADJENEK2020101592,
title = {Relevance- and interface-driven clustering for visual information retrieval},
journal = {Information Systems},
volume = {94},
pages = {101592},
year = {2020},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2020.101592},
url = {https://www.sciencedirect.com/science/article/pii/S0306437920300752},
author = {Mohamed Reda Bouadjenek and Scott Sanner and Yihao Du},
keywords = {Visual information retrieval, Relevance-driven Clustering, Visual search user study, Clustering via filter optimization},
abstract = {Search results of spatio-temporal data are often displayed on a map, but when the number of matching search results is large, it can be time-consuming to individually examine all results, even when using methods such as filtered search to narrow the content focus. This suggests the need to aggregate results via a clustering method. However, standard unsupervised clustering algorithms like K-means (i) ignore relevance scores that can help with the extraction of highly relevant clusters, and (ii) do not necessarily optimize search results for purposes of visual presentation. In this article, we address both deficiencies by framing the clustering problem for search-driven user interfaces in a novel optimization framework that (i) aims to maximize the relevance of aggregated content according to cluster-based extensions of standard information retrieval metrics and (ii) defines clusters via constraints that naturally reflect interface-driven desiderata of spatial, temporal, and keyword coherence that do not require complex ad-hoc distance metric specifications as in K-means. After comparatively benchmarking algorithmic variants of our proposed approach – RadiCAL – in offline experiments, we undertake a user study with 24 subjects to evaluate whether RadiCAL improves human performance on visual search tasks in comparison to K-means clustering and a filtered search baseline. Our results show that (a) our binary partitioning search (BPS) variant of RadiCAL is fast, near-optimal, and extracts higher-relevance clusters than K-means, and (b) clusters optimized via RadiCAL result in faster search task completion with higher accuracy while requiring a minimum workload leading to high effectiveness, efficiency, and user satisfaction among alternatives.},
pdf= {IS_101592.pdf},
preview={Elsevier.svg.png},
selected= {false},
abbr= {InfoSys},
}

@InProceedings{10.1007/978-3-030-73973-7_10,
author={Gupta, Shashank
and Robles-Kelly, Antonio
and Bouadjenek, Mohamed Reda},
editor={Torsello, Andrea
and Rossi, Luca
and Pelillo, Marcello
and Biggio, Battista
and Robles-Kelly, Antonio},
title={Feature Extraction Functions for Neural Logic Rule Learning},
booktitle={Structural, Syntactic, and Statistical Pattern Recognition},
year={2021},
publisher={Springer International Publishing},
address={Cham},
pages={98--107},
abstract={Combining symbolic human knowledge with neural networks provides a rule-based ante-hoc explanation of the output. In this paper, we propose feature extracting functions for integrating human knowledge abstracted as logic rules into the predictive behaviour of a neural network. These functions are embodied as programming functions, which represent the applicable domain knowledge as a set of logical instructions and provide a modified distribution of independent features on input data. Unlike other existing neural logic approaches, the programmatic nature of these functions implies that they do not require any kind of special mathematical encoding, which makes our method very general and flexible in nature. We illustrate the performance of our approach for sentiment classification and compare our results to those obtained using two baselines.},
isbn={978-3-030-73973-7},
pdf= {SSSPR_2020_paper_44.pdf},
preview={springer.png},
selected= {false},
abbr= {SSPR},
}

